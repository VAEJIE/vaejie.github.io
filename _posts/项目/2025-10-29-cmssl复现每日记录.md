---
layout: post
title: "cmssl复现每日记录"
description: ""
categories: "项目"
---
## 微调记录
ecg的用ecg的权重full_finetunig得到的结果和论文几乎一样
最好的：
ptb-xl,0.6025,0.8184,0.8259,,,0.6091,,,,
physionet2017,0.4367,0.7404,,0.4311,0.2732,0.6029,,,,
csn,0.1687,0.7855,0.9881,,,0.5539,,,,
#### ppg的数据集：
##### bidmc_hr:
    1.用full_finetuning ， lr=1e-4， backbone_lr=1e-5 得到的bidmc_hr,,,,,,,2.8938,0.9125,3.8444,
    第17epoch就停了，之前的trainloss也是一直在减小，减小到了0.5083，但是val loss是在变大的，一直大到2.8118，明显过拟合了

    2.用linear_probing,   lr=1e-4， backbone_lr=1e-5 得到的bidmc_hr,,,,,,,7.6351,0.4876,9.3058,太差劲了，epoch到48/1024，trainloss是从1左右下降到0.69，valloss从2.9左右开始下降到epoch13后valloss到了2.1左右，后面的epoch就有时候上升有时候下降反正就在2左右摇摆

    3.重新用回full，lr = 1e-5, backbone_lr = 1e-6, 得到encoder,bidmc_hr,,,,,,,3.5531,0.8811,4.4830，还不如第一次的

    4.用full，lr = 1e-4，backbonelr = 1e-5（源代码的参数），与源代码差别就是改了个loss，把mseloss改成L1loss了，得到：bidmc_hr,,,,,,,2.1027,0.9590,2.6314 **最好的一次**

    5.用full，lr = 1e-4，backbonelr = 2e-5，bidmc_hr,,,,,,,1.8753,0.9628,2.5090，**新的最好的**
    这次就是把backbonelr调大了一点，backbone轻微解冻了能更好的融合任务特征

    6. full,lr = 1e-4, backbone_lr = 3e-5，bidmc_hr,,,,,,,1.8924,0.9648,2.4396 **变化不大了但与论文还是差距较大**
   
    7. full,lr = 1e-4, backbone_lr = 1.5e-5,bidmc_hr,,,,,,,1.8963,0.9663,2.3869

    8.full,lr = 1e-4, backbone_lr = 1.2e-5 , bidmc_hr,,,,,,,1.9702,0.9632,2.4933
        >*我一开始学习率较高，发现 Val Loss 暴涨，判断是过拟合/不稳定，所以我降低了学习率并改用 L1 Loss 增强稳定性。后来发现学习率过低导致性能不佳，所以我又逐步提高学习率，特别是调整了 Backbone 和 Head 的学习率比例，最终找到了现在这个效果最好的组合*
##### 最后还是用了full，lr = 1e-4，backbonelr = 2e-5，bidmc_hr,,,,,,,1.8753,0.9628,2.5090
师兄说可能是随机种子，说是要调优的话还要上贝叶斯，要跑很多轮......呃..我也不知道了

##### bidmc_rr:
    1.full , lr = 1e-4, backbonelr = 2e-5, bidmc_rr,,,,,,,3.6608,-0.9294,5.3737   和论文的**2.35、3.08**相差不少
    2. full , lr = 1e-4, backbonelr = 1e-5 ,bidmc_rr,,,,,,,2.4108,-0.0676,3.9973 **和论文相差小**
##### ppg_dalia_hr:
    1.full , lr = 1e-4,backbone_lr = 1e-5, ppg_dalia_hr,,,,,,,7.6117,0.3173,13.9696 , MAE比论文的还好，这是误差影响吗，是正常的吗，RMSE和论文的12.57比较接近
    2.full , lr = 1e-4,backbone_lr = 2e-5,ppg_dalia_hr,,,,,,,6.4314,0.4897,12.0776 , 额....比sota还好，感觉不对,不行了，我没招了
##### ppg_dalia_rr:
    1.full , lr = 1e-4,backbone_lr = 1e-5, ppg_dalia_rr,,,,,,,0.0970,-0.0710,0.1153  和论文很接近了，不错

uci 的数据转换的代码是自己写的，源代码缺失
##### uci_sbp:
    1. lr=1e-4 backbone_lr = 1e-5,，uci_sbp,,,,,,,12.1930,0.3772,17.8499
    2. 用backbonelr = 2e-5试试 ，不好
##### uci_dbp:
    1. lr=1e-4 backbone_lr = 2e-5, uci_dbp,,,,,,,5.8991,0.3230,8.9798  和论文的5.88  9.05十分接近甚至略好，我想是不是lr的原因所以下次把backbone_lr改为一开始的1e-5试试
    2. lr=1e-4 backbone_lr = 1e-5, uci_dbp,,,,,,,5.8179,0.3383,8.8773，额。。。。。没招了





看了encoder_finetuning代码

## 调参的学习
#### lr 和 backbone_lr
##### lr是指新添加的预测头（Head）的学习率，要从头开始学习如何把 Backbone 提取的特征映射到最终的预测结果
##### backbone_lr 是预训练好的骨干网络的学习率
通常lr 比 backbone_lr大， 因为##### 学习率控制模型参数更新的步子大小
    >给 Head 一个较大的步子让它快速入门新任务；给 Backbone 一个较小的步子让它在原有知识基础上“小心翼翼”地微调，避免“步子太大扯到蛋”——也就是灾难性遗忘 (Catastrophic Forgetting)，把预训练学好的东西忘光了

