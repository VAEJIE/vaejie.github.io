---
layout: post
title: ""
description: "Reducing the Dimensionality of Data with Neural Networks"
categories: "论文"
---
降维有助于高维数据的分类、可视化、通信和存储
## 概念
AE : Autoencoder 自编码器
encoder:“压缩”，降维
**压缩的低维向量即“Code”，也就后面常说的“潜在表示”或“潜在空间”**
decoder：“解压”，重建

--------------------------------------------
## 痛点
传统反向传播，链式法则求损失函数对任一权重的偏导，梯度计算过程其实就是极多个的激活函数的偏导数乘以权重的这么多项的连乘积，由于激活函数的偏导数可能是远小于1（如sigmoid）经过很多层后连乘约等于0了或者当z的值比较大，经过激活函数饱和，任何一层的z的激活函数的导数都几乎为0，这都会造成**梯度消失**，使得浅层网络的权重（参数）几乎得不到更新，导致整个深度网络无法收敛到一个好的解

题外话，回到此式子$\prod_{k=2}^{n} \left( \sigma'(z_k) \cdot W_k \right)$，当Wk很大时，乘积持续大于1，会造成**梯度爆炸**

是一种非线性的降维工具，远比PCA（线性降维）要好
------------------------------------
## 核心贡献
1. 逐层预训练 (Layer-wise Pretraining)
使用受限玻尔兹曼机 (RBM) 的模块

2. 展开（Unrolling）和微调（Fine-tuning）
预训练和展开后已经使这个深度AE的权重已经处在一个很好的初始位置了，
此时，再使用标准的反向传播算法，以“最小化重建损失”为目标，对整个网络的所有权重进行“微调”
![alt text](/images/posts/论文项目/AEFif.1.png)
