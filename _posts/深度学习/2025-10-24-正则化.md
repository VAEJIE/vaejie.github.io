---
layout: post
title: "正则化"
description: ""
categories: "深度学习"
---

### 正则化是防止模型过拟合的技术
### 过拟合就是模型学到了训练集中的噪声和偶然性，泛化能力差

## 方法
#### L1和L2正则化：在模型的原始损失函数（比如交叉熵或MSE）上，额外加上一个惩罚项，这个惩罚项与模型权重的大小有关
L1  ：惩罚项是所有权重绝对值之和 ($||\mathbf{w}||_1 = \sum |w_i|$)。它倾向于让模型权重变得稀疏（很多权重变成精确的0），可以用于特征选择
L2  ：惩罚项是所有权重平方和之和 ($||\mathbf{w}||_2^2 = \sum w_i^2$)。它倾向于让模型的权重整体都比较小，但不一定为0。这是深度学习中最常用的正则化方法之一，通常称为**“权重衰减 (Weight Decay)”**
#### dropout ： 随机的让部分神经元失活
#### 数据增强：对图像随机裁剪、旋转、翻转、颜色抖动；对信号加随机噪声、时间轴缩放/扭曲、随机遮蔽、信号反转、通道混合等
#### 提前终止（early stopping）
在训练过程中，周期性地在验证集上评估模型性能，在验证集性能最佳点停下来
#### 标签平滑
在分类任务中，将**“硬”的目标标签**（比如正确类别为1，其他为0）替换为**“软”的标签**（比如正确类别为 $1-\epsilon$，其他类别平分 $\epsilon$）


## 残差连接： 旨在解决训练非常深的神经网络时的**“梯度消失/爆炸”和“网络退化”**问题
#### ResNet
不需要拟合一个完整的复杂函数，而是在恒等映射x的基础上进行修正和微调，因为F（x）可以=0
![alt text](/images/posts/论文项目/残差连接.png)













