---
layout: post
title: "Masked Autoencoders Are Scalable Vision Learners"
description: 
categories: "论文"
---
## 总
#### 高掩码率
#### 非对称式架构
#### 大道至简————“遮+重建”
#### 范式统一
-----------------------------

## ViT      Vision Transformer
#### 创新点
##### 1. 图像转化为Token序列：AN IMAGE IS WORTH 16X16 WORDS 
##### 2. 架构设计：
    - 把图像信息转成语义信息：
![alt text](/images/posts/论文项目/to_patch.png)
        1.分块（Patching）：如果patch_size = 16 x 16,那么224 x 224的图变成了共（（224➗16=14）✖️（224➗16=14）= 196块）的小拼图patches，每个patch大小是16✖️16✖️3=768（RGB3个通道）
        2.嵌入（Embedding）：
            1.展平
            2.线性映射：把像素数据翻译成语义向量
        3.加入CLS Token和位置编码
        ![alt text](/images/posts/论文项目/1.png)
    - 送入Transformer的encoder










#### 痛点————归纳偏置（小数据是优点，大数据是缺点）
    1. **局部性**：CNN通过卷积核（kernel）进行运算，它天生假设图像信息是局部的。一个神经元只关注其“感受野”内的一小块区域
    2. **平移不变性**：CNN假设图像中的某个模式（比如一只鸟）无论出现在左上角还是右下角，都应该被同一个卷积核检测出来


----------------------------

### 在生理信号领域有**MSM**
ST-MEM 、MMAE-ECG等论文都是MAE在ECG信号上的直接应用

