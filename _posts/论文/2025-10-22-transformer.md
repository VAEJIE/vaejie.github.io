---
layout: post
title: "Transformer"
description: 
categories: "论文"
---
## 痛点
RNN循环神经网络无法在序列内部并行计算、长程依赖难捕捉,$t$时刻的隐藏状态 $h_t$ 依赖于 $t-1$ 时刻的隐藏状态 $h_{t-1}$ 



-----------------------------------
## Encoder-Decoder 架构
![alt text](/images/posts/论文项目/transformer.png)
堆叠的自我注意力和逐点全连接层作为编码器和解码器

#### Encoder
由N = 6个相同的层堆叠，每层有两个子层：
1. **多头自注意力机制**
2. **逐位置的前馈神经网络**
##### 关键架构： 
两个都用了**残差连接**和**层归一化**![alt text](/images/posts/论文项目/encoder.png)

每个子层的最终输出是：$LayerNorm(x + Sublayer(x))$ 
*（专家提示：这是训练深度Transformer至关重要的技巧，能确保梯度稳定传播)*

#### Decoder
由 N = 6 个相同的层堆叠而成，但每一层有三个子层：
1. **带掩码的多头自注意力**
2. **多头“编码器-解码器”注意力**
3. **逐位置的前馈神经网络**
##### 关键架构：
1. **掩码的自注意力**：不许偷看“未来”
2. **“编码器-解码器”注意力（交叉注意力）**：
    - 这是连接编码器和解码器的桥梁。
    - 它的Query（查询） 来自上一个解码器子层 。
    - 它的Key（键） 和 Value（值） 来自编码器的最终输出 。
    - 这使得解码器中的每个位置都能关注到输入序列中的所有位置


------------------------------------
## 注意力机制
#### 缩放点积注意力  ![alt text](/images/posts/论文项目/缩放点积注意力.png)
   - 输入：查询 $Q$（Query）、键 $K$（Key）的维度为 $d_k$ 37，值 $V$（Value）的维度为 $d_v$
   - 原理：
       1. 计算 $Q$ 和 $K$ 的点积，得到“相似度”分数。
       2. 【关键细节：缩放】 将分数除以 $\sqrt{d_k}$ 
       3. 应用 Softmax 将分数归一化为权重 
       4. 将权重与 $V$ 进行加权求和，得到输出 
![alt text](images/posts/论文项目/注意力1公式.png)
