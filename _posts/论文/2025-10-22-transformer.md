---
layout: post
title: "Transformer"
description: 
categories: "论文"
---
## 痛点
RNN循环神经网络无法在序列内部并行计算、长程依赖难捕捉,$t$时刻的隐藏状态 $h_t$ 依赖于 $t-1$ 时刻的隐藏状态 $h_{t-1}$ 



-----------------------------------
## Encoder-Decoder 架构
![alt text](/images/posts/论文项目/transformer.png)
堆叠的自我注意力和逐点全连接层作为编码器和解码器

### Encoder
由N = 6个相同的层堆叠，每层有两个子层：
- **多头自注意力机制**
- **逐位置的前馈神经网络**
#### 关键架构： 
每两个子层都用了**残差连接**和**层归一化**![alt text](/images/posts/论文项目/encoder.png)

每个子层的最终输出是：$LayerNorm(x + Sublayer(x))$ 
*（专家提示：这是训练深度Transformer至关重要的技巧，能确保梯度稳定传播)*

### Decoder
由 N = 6 个相同的层堆叠而成，但每一层有三个子层：
1. **带掩码的多头自注意力**
2. **多头“编码器-解码器”注意力**
3. **逐位置的前馈神经网络**
    #### 关键架构：
      - **掩码的自注意力**：不许偷看“未来”
      - **“编码器-解码器”注意力（交叉注意力）**：
         - 这是连接编码器和解码器的桥梁。
         - 它的Query（查询） 来自上一个解码器子层 。
         - 它的Key（键） 和 Value（值） 来自编码器的最终输出 。
         - 这使得解码器中的每个位置都能关注到输入序列中的所有位置


### 注意力机制
#### 缩放点积注意力  
![alt text](/images/posts/论文项目/缩放点积注意力.png)
   - 输入：查询 $Q$（Query）、键 $K$（Key）的维度为 $d_k$ 37，值 $V$（Value）的维度为 $d_v$
   - 原理：
       1. 计算 $Q$ 和 $K$ 的点积，得到“相似度”分数。
       2. 【关键细节：缩放】 将分数除以 $\sqrt{d_k}$ 
       3. 应用 Softmax 将分数归一化为权重 
       4. 将权重与 $V$ 进行加权求和，得到输出 
![alt text](/images/posts/论文项目/注意力1公式.png)

#### 多头注意力
![alt text](/images/posts/论文项目/多头.png)
- 工作原理：
    1. 将 $Q, K, V$ 通过不同的、可学习的线性投影（$W_i^Q, W_i^K, W_i^V$），分别投影 $h$ 次 
    2. 并行地对这 $h$ 组投影后的 $Q_i, K_i, V_i$ 执行缩放点积注意力，得到 $h$ 个输出 $head_i$ 
    3. 拼接（Concat） 这 $h$ 个输出：$Concat(head_1, ..., head_h)$ 
    4. 将拼接后的结果再通过一个线性投影（$W^O$），得到最终输出 

#### 三处自注意
##### 1. 编码器自注意力
Q，K，V全来自上一层编码器的输出
##### 2. 解码器掩码自注意力
Q，K，V全来自上一层解码器的输出
##### 3. 编码器-解码器自注意力
位于解码器子层2，Q来自子层1的解码器掩码自注意力的输出，KV来自编码器的*最终输出*（N个层的最顶层的输出，KV代表了原始输入句子的完整信息）


### 逐位置的FNN
![alt text](/images/posts/论文项目/FNN.png)
细节：虽然它在不同位置（如词1、词2）上是相同的函数（即 $W, b$ 共享），但在不同层之间，它的参数是不同的

### 位置编码（Postional Encoding）
![alt text](/images/posts/论文项目/Postional Encoding.png)



