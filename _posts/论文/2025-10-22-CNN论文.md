---
layout: post
title: "ImageNet Classification with Deep Convolutional Neural Networks"
description: 
categories: "论文"
---
{% include JB/setup %}
## 价值所在
不在具体的网络结构，而是提出了一整套的“组合拳”
- **ReLU激活函数**：“非饱和”函数
- **正则化**：**dropout**：在训练时，以0.5的概率随机“丢弃”（输出清零）全连接层中的一半神经元和**数据增强**：通过对图像进行随机裁剪、水平翻转 和 PCA颜色抖动，对信号加随机噪声、时间缩放、随机遮蔽
- 跨GPU并行


## CNN
#### 卷积层（Convolutional Layer）
卷积核、滤波器， 学习*“模式”*
1. 局部感受
2. 参数共享：同样的模型用一个核检测出来
3. 层次化特征提取

#### 池化层（Pooling layer）：缩小范围，让下一个神经元有更大的感受野
max pooling：把特征图划分成若干个小区域（比如 $2 \times 2$），然后在每个区域里只保留最大的那个数值，丢弃其他的
作用：1. 降维 2. 引入不变性
AlexNet使用了重叠池化 (Overlapping Pooling)：池化窗口大小 > 步长
![alt text](/images/posts/论文项目{524F9558-0853-43B7-B1E2-3550234B6D51}.png)

