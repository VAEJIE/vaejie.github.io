---
layout: post
title: "鸢尾花分类任务优化器对比实验"
description: ""
categories: "深度学习"
---
{% include JB/setup %}
# 鸢尾花分类任务优化器对比实验：核心知识点总结

## 一、核心思想：下山策略对比

想象一下，Beale函数是一座山的地形图，我们的目标是走到最低点 `(3, 0.5)`。优化器就是我们蒙着眼睛下山的策略。

- **无优化器**: 原地踏步，永远到不了山谷。
- **SGD (随机梯度下降)**: 最朴素的策略。只看脚下哪边最陡，就朝反方向迈一小步。
- **Momentum (动量)**: 像一个滚下山的铁球。除了看当前坡度，还会因为**惯性**继续向前冲，这能帮助越过小障碍。
- **Adagrad (自适应梯度)**: 聪明的登山者。在**陡峭处迈小步**（防止冲过头），在**平缓处迈大步**（加速前进）。


---

## 二、优化器原理 

#### 1. SGD (Stochastic Gradient Descent)
- **核心**: 参数沿当前梯度**相反**的方向更新。
- **公式**: `新位置 = 旧位置 - 学习率 * 梯度`
$$
\theta_{new} = \theta_{old} - \eta \cdot \nabla_{\theta}J(\theta)
$$
- **记忆点**: 简单直接，但易陷入局部最优，且收敛慢。

#### 2. Momentum (动量)
- **核心**: 引入“速度”向量 `v`，是历史梯度的加权平均。更新时同时考虑**惯性**和**当前梯度**。
- **公式**:
  `速度 = 动量系数 * 旧速度 + 学习率 * 梯度`
  `新位置 = 旧位置 - 速度`
$$
v_{t} = \gamma \cdot v_{t-1} + \eta \cdot \nabla_{\theta}J(\theta)
$$
$$
\theta_{new} = \theta_{old} - v_{t}
$$
- **记忆点**: 惯性加速收敛，有助于“冲出”局部最优点。

#### 3. Adagrad (Adaptive Gradient)
- **核心**: 为**每一个参数**维护一个**自适应**的学习率。
- **公式**:
  `历史梯度平方和 r += 梯度^2`
  `新位置 = 旧位置 - (基础学习率 / sqrt(r)) * 梯度`
  $r_{t} = r_{t-1} + (\nabla_{\theta}J(\theta))^2$
  $\theta_{new} = \theta_{old} - \frac{\eta}{\sqrt{r_t} + \epsilon} \cdot \nabla_{\theta}J(\theta)$
- **记忆点**: 梯度大的参数学习率自动变小，梯度小的参数学习率自动变大。适合稀疏数据，但后期学习率可能过小。

---

## 三、重要的代码实现解析

```python
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.colors as plt_cl

# 1. 构建Beale公式及其导数
# 定义beale公式
def beale(x1,x2):
    return (1.5-x1+x1*x2)**2+(2.25-x1+x1*x2**2)**2+(2.625-x1+x1*x2**3)**2

# 定义beale公式的偏导函数
def dbeale_dx(x1, x2):
    dfdx1 = 2*(1.5-x1+x1*x2)*(x2-1)+2*(2.25-x1+x1*x2**2)*(x2**2-1)+2*(2.625-x1+x1*x2**3)*(x2**3-1)
    dfdx2 = 2*(1.5-x1+x1*x2)*x1+2*(2.25-x1+x1*x2**2)*(2*x1*x2)+2*(2.625-x1+x1*x2**3)*(3*x1*x2**2)
    return dfdx1, dfdx2

# 2. 定义画图函数和准备画布
def gd_plot(x_traj, title):
    plt.rcParams['figure.figsize'] = [6, 6]
    plt.contour(X1, X2, Y, levels=np.logspace(0, 6, 30),
                norm=plt_cl.LogNorm(), cmap=plt.cm.jet)
    plt.title(f'2D Contour Plot - {title}')
    plt.xlabel('$x_1$')
    plt.ylabel('$x_2$')
    plt.axis('equal')
    plt.plot(3, 0.5, 'k*', markersize=10) # 目标点
    if x_traj is not None:
        x_traj = np.array(x_traj)
        plt.plot(x_traj[:, 0], x_traj[:, 1], 'k-')
    plt.show()

# 创建网格用于可视化
step_x1, step_x2 = 0.2, 0.2
X1, X2 = np.meshgrid(np.arange(-5, 5 + step_x1, step_x1),
                     np.arange(-5, 5 + step_x2, step_x2))
Y = beale(X1,X2)

print("目标结果(x_1, x_2) = (3, 0.5)")
gd_plot(None, "Beale Function Surface")


# 3. 无优化器
def gd_no(df_dx, x0, conf_para=None):
    if conf_para is None:
        conf_para = {}
    conf_para.setdefault('n_iter', 1000)
    conf_para.setdefault('learning_rate', 0.001)
    x_traj = []
    x_traj.append(x0)
    # 没有迭代更新的操作，所以坐标没有变化
    for iter in range(1, conf_para['n_iter'] + 1):
        x_traj.append(x_traj[-1])
    return x_traj

print("\n--- 无优化器 ---")
x0_no = np.array([1.0, 1.5])
conf_para_no = {'n_iter': 2000, 'learning_rate': 0.005}
x_traj_no = gd_no(dbeale_dx, x0_no, conf_para_no)
print("无优化器求得极值点 (x_1, x_2) = (%s, %s)" % (x_traj_no[-1][0], x_traj_no[-1][1]))
gd_plot(x_traj_no, "No Optimizer")


# 4. SGD优化器
def gd_sgd(df_dx, x0, conf_para=None):
    if conf_para is None:
        conf_para = {}
    conf_para.setdefault('n_iter', 1000)
    conf_para.setdefault('learning_rate', 0.001)
    x_traj = []
    x_traj.append(x0)
    v = np.zeros_like(x0)
    # 利用梯度值对坐标进行更新
    for iter in range(1, conf_para['n_iter'] + 1):
        dfdx = np.array(df_dx(x_traj[-1][0], x_traj[-1][1]))
        v = - conf_para['learning_rate'] * dfdx
        x_traj.append(x_traj[-1] + v)
    return x_traj

print("\n--- SGD 优化器 ---")
x0_sgd = np.array([1.0, 1.5])
conf_para_sgd = {'n_iter': 2000, 'learning_rate': 0.005}
x_traj_sgd = gd_sgd(dbeale_dx, x0_sgd, conf_para_sgd)
print("SGD求得极值点 (x_1, x_2) = (%s, %s)" % (x_traj_sgd[-1][0], x_traj_sgd[-1][1]))
gd_plot(x_traj_sgd, "SGD Optimizer")


# 5. Momentum优化器
def gd_momentum(df_dx, x0, conf_para=None):
    if conf_para is None:
        conf_para = {}
    conf_para.setdefault('n_iter', 1000)
    conf_para.setdefault('learning_rate', 0.001)
    conf_para.setdefault('momentum', 0.9)
    x_traj = []
    x_traj.append(x0)
    v = np.zeros_like(x0)
    # 套用动量优化器公式，对坐标值进行更新
    for iter in range(1, conf_para['n_iter'] + 1):
        dfdx = np.array(df_dx(x_traj[-1][0], x_traj[-1][1]))
        v = conf_para['momentum'] * v - conf_para['learning_rate'] * dfdx
        x_traj.append(x_traj[-1] + v)
    return x_traj

print("\n--- Momentum 优化器 ---")
x0_momentum = np.array([1.0, 1.5])
conf_para_momentum = {'n_iter': 500, 'learning_rate': 0.005}
x_traj_momentum = gd_momentum(dbeale_dx, x0_momentum, conf_para_momentum)
print("Momentum求得极值点 (x_1, x_2) = (%s, %s)" % (x_traj_momentum[-1][0], x_traj_momentum[-1][1]))
gd_plot(x_traj_momentum, "Momentum Optimizer")


# 6. Adagrad优化器
def gd_adagrad(df_dx, x0, conf_para=None):
    if conf_para is None:
        conf_para = {}
    conf_para.setdefault('n_iter', 1000)
    conf_para.setdefault('learning_rate', 0.001)
    conf_para.setdefault('epsilon', 1e-7)
    x_traj = []
    x_traj.append(x0)
    r = np.zeros_like(x0)
    # 套用adagrad优化器公式，对参数进行更新
    for iter in range(1, conf_para['n_iter'] + 1):
        dfdx = np.array(df_dx(x_traj[-1][0], x_traj[-1][1]))
        r += dfdx ** 2
        update = - conf_para['learning_rate'] / (np.sqrt(r) + conf_para['epsilon']) * dfdx
        x_traj.append(x_traj[-1] + update)
    return x_traj

print("\n--- Adagrad 优化器 ---")
x0_adag = np.array([1.0, 1.5])
conf_para_adag = {'n_iter': 500, 'learning_rate': 2}
x_traj_adag = gd_adagrad(dbeale_dx, x0_adag, conf_para_adag)
print("Adagrad求得极值点 (x_1, x_2) = (%s, %s)" % (x_traj_adag[-1][0], x_traj_adag[-1][1]))
gd_plot(x_traj_adag, "Adagrad Optimizer")
```