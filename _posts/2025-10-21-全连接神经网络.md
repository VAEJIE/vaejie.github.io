---
layout: post
title: "全连接神经网络"
description: ""
categories: "深度学习"
---
## 3蓝1棕
![alt text](/images/posts/深度学习/image1.jpg)
bias的目的是约束加权和，表明神经元是否更容易被激活
![alt text](/images/posts/深度学习/image2.jpg)

MNIST的784的输入的这个网络有13000多个权重和偏置值，正是这些权重和偏置决定了网络
------------------------------------------

## PyTorch FNN代码

```py
import torch
import torch.nn as nn
import torch.nn.functional as F

class SimpleFNN(nn.Module):
    def __init__(self, input_size, hidden_szie, num_classes):
        super(SimpleFNN,self).__init__()

       self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, hidden_size)
        self.fc3 = nn.Linear(hidden_size, num_classes)
    def forward(self,x):
        # x 初始形状: [N, 1, 28, 28] (N是批量大小)
        # 1. 展平 (Flatten): [N, 1, 28, 28] -> [N, 784]
        #    -1 会自动计算 N 的大小
        x = x.view(-1, 784)
        x = self.fc1(x)                 #self是实例对象
        x = F.relu(x)

        x = self.fc2(x)
        s = F.relu(x)

        x = self.fc3(x)
        return x
```


## 训练循环

```py
# --- 假设您已经准备好了这些 (这些是“套话”，不是核心逻辑) ---
#model = SimpleFNN(...)
# train_loader = ... (数据加载器)
# criterion = nn.CrossEntropyLoss()    # 损失函数 (用交叉熵)
# optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # 优化器 (用Adam)
# model.train() # 开启训练模式
# 【必背】训练循环
def train_one_epoch(model, train_loader, criterion, optimizer):
    
    # 遍历数据集
    for data, targets in train_loader:
        # data, targets = data.to(device), targets.to(device) # (1. 放到GPU)

        # 【必背五步法】
        # 1. 梯度清零
        optimizer.zero_grad()
        # 2. 前向传播 (跑模型)
        outputs = model(data)
        # 3. 计算损失 (和标签对比)
        loss = criterion(outputs,targets)
        # 4. 反向传播 (自动算梯度)
        loss.backward()
        # 5. 更新权重 (应用梯度)
        optimizer.step()
        
        # print(f"Loss: {loss.item()}") # (可选：打印损失)
```

